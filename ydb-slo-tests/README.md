# SLO workload

SLO is the type of test where app based on ydb-sdk is tested against falling YDB cluster nodes, tablets, network
(that is possible situations for distributed DBs with hundreds of nodes)

### Usage:

It has 3 commands:

- `create`  - creates table in database
- `cleanup` - drops table in database
- `run`     - runs workload (read and write to table with sets RPS)

### Run examples with all arguments:

create:

`cargo run --example native grpc://localhost:2136 /local tableName create --min-partitions-count 6 --max-partitions-count 1000 --partition-size 1 -c 1000 --write-timeout 10000`

cleanup:

`cargo run --example native grpc://localhost:2136 /local tableName cleanup`

run:

`cargo run --example native grpc://localhost:2136 /local tableName run -c 1000 --read-rps 1000 --read-timeout 10000 --write-rps 100 --write-timeout 10000 --time 600 --prom-pgw localhost:9091 --report-period 250`

## Arguments for commands:

### create

`cargo run --example <example_name> <ENDPOINT> <DB> <TABLE_NAME> create [OPTIONS]`

```
Arguments:
  ENDPOINT                            YDB endpoint to connect to
  DB                                  YDB database to connect to
  TABLE_NAME                          table name to create

Options:
  --min-partitions-count     <u64>    minimum amount of partitions in table
  --max-partitions-count     <u64>    maximum amount of partitions in table
  --partition-size           <u64>    partition size in mb
  
  -c --initial-data-count    <u64>    amount of initially created rows
  
  --write-timeout            <u64>    write timeout milliseconds
```

### cleanup

`cargo run --example <example_name> <ENDPOINT> <DB> <TABLE_NAME> cleanup`

```
Arguments:
  ENDPOINT    YDB endpoint to connect to
  DB          YDB database to connect to
  TABLE_NAME  table name to cleanup
```

### run

`cargo run --example <example_name> <ENDPOINT> <DB> <TABLE_NAME> run`

```
Arguments:
  ENDPOINT                            YDB endpoint to connect to
  DB                                  YDB database to connect to
  TABLE_NAME                          table name to use

Options:
  -c --initial-data-count    <u64>    amount of initially created rows
                         
  --read-rps                 <u32>    read RPS
  --read-timeout             <u64>    read timeout milliseconds
                         
  --write-rps                <u32>    write RPS
  --write-timeout            <u64>    write timeout milliseconds
                         
  --time                     <u64>    run time in seconds
  
  --prom-pgw                 <string> prometheus push gateway
  --report-period            <u64>    prometheus push period in milliseconds
```

## What's inside

When running `run` command, the program creates two jobs: `readJob`, `writeJob`, `metricsJob`.

- `readJob`  reads rows from the table one by one with random identifiers generated by `writeJob`
- `writeJob` generates and inserts rows
- `metricsJob` periodically sends metrics to Prometheus

Table have these fields:

- `hash Uint64`
- `id Uint64`
- `payload_str Text?`
- `payload_double Double?`
- `payload_timestamp Timestamp?`
- `payload_hash Uint64?`

Primary key: `("hash", "id")`

## Collected metrics

- `sdk_errors_total`              - Total number of errors encountered, categorized by error type
- `sdk_operations_total`          - Total number of operations, categorized by type attempted by the SDK
- `sdk_operations_success_total`  - Total number of successful operations, categorized by type
- `sdk_operations_failure_total`  - Total number of failed operations, categorized by type
- `sdk_operation_latency_seconds` - Latency of operations performed by the SDK in seconds, categorized by type and
  status
- `sdk_retry_attempts`            - Current retry attempts, categorized by operation type
- `sdk_retry_attempts_total`      - Total number of retry attempts, categorized by operation type
- `sdk_retries_success_total`     - Total number of successful retries, categorized by operation type
- `sdk_retries_failure_total`     - Total number of failed retries, categorized by operation type
- `sdk_pending_operations`        - Current number of pending operations, categorized by type

## Look at metrics in grafana

You can get dashboard used in that
test [here](https://github.com/ydb-platform/slo-tests/blob/main/k8s/helms/grafana.yaml#L69) - you will need to import
json into grafana.
